
ğŸ“Š Benchmark Report (Short & Simple)

This report summarizes the performance of the **CPU-based RAG pipeline** using **Mistral-7B-Instruct (GGUF)**, FAISS, and Sentence-Transformers.

---

ğŸ–¥ï¸ System Used

* CPU: Intel i5/i7
* RAM: 8â€“16 GB
* GPU: None (CPU-only)
* Model: Mistral-7B-Instruct-Q4_K_M
* Embedding Model: MiniLM-L6-v2

---

ğŸš€ Key Performance Metrics

| Component                       | Time              
| ------------------------------- | ----------------- 
| **Model Load**                  | 11â€“15 seconds     
| **FAISS Index Load**            | 0.15â€“0.25 seconds 
| **Retrieval Time**              | 20â€“40 ms          
| **LLM Generation (150 tokens)** | 3â€“5 seconds       
| **End-to-End RAG Response**     | 5â€“7 seconds   

---

ğŸ§ª Example RAG Query

Question:
What does the assignment require?

Performance:

* Retrieval: ~30 ms
* Generation: ~4 seconds
* Total: ~5 seconds

---

âœ” Observations

* Works fully offline on CPU
* Retrieval is extremely fast
* LLM generation is the slowest part (expected on CPU)
* End-to-end latency is **5â€“7 seconds**, suitable for demos or interviews

---

âš™ Simple Optimizations

* Reduce `max_tokens`
* Reduce `n_ctx` from 4096 â†’ 2048
* Use 6â€“8 CPU threads
* Use smaller models (Phi-3, Mistral-7B Q4_K_S)

