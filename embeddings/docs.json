["Task Title: \nDevelop CPU-Based RAG LLM Inference Pipeline \nObjective: \nDesign and implement a Retrieval-Augmented Generation (RAG) pipeline optimized for \ninference on CPU-only environments. The solution should enable efficient document \nretrieval and response generation using a lightweight LLM and embedding model. \nResponsibilities: \n1. Design the Architecture: \n \n\u25cb Define a modular pipeline integrating document retrieval and generative \nresponse components. \n \n\u25cb Ensure all components are optimized for CPU usage with minimal memory \nand latency footprint. \n \n2. Embed and Store Documents: \n \n\u25cb Use a CPU-compatible embedding model (e.g., sentence-transformers, \nMiniLM,llamafile etc.). \n \n\u25cb Store embeddings in a vector database (e.g., FAISS with CPU backend, \nChromaDB). \n \n3. Implement Retrieval: \n \n\u25cb Perform efficient similarity search using CPU-based vector index. \n \n\u25cb Implement ranking logic if necessary to refine the top-k results. \n \n4. Integrate LLM Inference: \n \n\u25cb Use a lightweight CPU-optimized LLM (llama.cpp, ggml,llamafile,vllm). \n \n\u25cb Implement prompt construction by combining query and retrieved documents. \n \n5. Pipeline Integration: \n \n\u25cb Build an inference API or script that accepts a user query, retrieves relevant \ndocuments, and returns a generated response. \n ", "\u25cb Ensure the pipeline runs within acceptable latency bounds on CPU. \n \n6. Testing and Benchmarking: \n \n\u25cb Evaluate response accuracy, retrieval quality, and latency. \n \n\u25cb Compare performance across different CPU models if needed. \n \n7. Documentation: \n \n\u25cb Provide clear usage instructions and installation requirements. \n \n\u25cb Document the pipeline structure, model choices, and optimization techniques. \n \nDeliverables: \n\u25cf End-to-end CPU-based RAG inference pipeline (codebase). \n \n\u25cf Sample script/API or streamlit for interactive use. \n \n\u25cf Deployment guide and benchmark report. \n \n\u25cf Documentation for extending or modifying the pipeline. \n "]