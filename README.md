# CPU-Optimized RAG LLM â€” End-to-End Project


ğŸ§  CPU-based RAG Pipeline (PDF Question Answering)

This project implements a complete **Retrieval-Augmented Generation (RAG)** pipeline using:

- PDF ingestion
- SentenceTransformer embeddings
- FAISS vector database
- Local LLM (Mistral 7B Instruct, GGUF) using llama.cpp
- CPU-only execution
- Chat-based question answering

It allows you to upload a PDF and ask questions like:

> â€œWhat does the assignment require?â€

The model retrieves relevant pages from the PDF and generates an accurate answer.

---

ğŸš€ Features

- Extracts text from PDF page-wise  
- Generates embeddings using `all-MiniLM-L6-v2`  
- Stores vectors using FAISS  
- Loads a local Mistral 7B model via llama.cpp  
- Supports chat-style prompt using built-in template  
- Fully CPU-compatible  
- Clean modular structure (ingest â†’ retrieve â†’ generate â†’ pipeline)

---

ğŸ— Architecture

                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚         PDF Document        â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                       (1) Ingestion
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     Sentence Embeddings     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                     (2) Store in FAISS
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚      Vector Database        â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                       (3) Retrieve top-K
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  RAG Prompt Construction    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                     (4) Generate Answer
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Local LLM (Mistral 7B)   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



---
ğŸ“¦ Folder Structure

cpu_rag_pipeline/
â”‚
â”œâ”€â”€ data/docs/AI_ML Assignment.pdf
â”œâ”€â”€ embeddings/
â”‚     â”œâ”€â”€ faiss.index
â”‚     â”œâ”€â”€ docs.json
â”‚
â”œâ”€â”€ models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ retrieve.py
â”‚   â”œâ”€â”€ generate.py
â”‚   â””â”€â”€ pipeline.py
â”‚
â””â”€â”€ README.md

````

---
ğŸ”§ Installation (Windows)

cd "C:\Users\91637\Downloads\cpu_rag_package\cpu_rag_pipeline"

1. Create environment

python -m venv venv
venv\Scripts\activate


2. Install dependencies

pip install -r requirements.txt


3. Download Mistral Model (GGUF)

## mistral-7b-instruct-v0.2.Q4_K_M.gguf ##

Place it in:

models/




ğŸ›  Step 1 â€” Build the Vector Store

python src/ingest.py


This will create:

```
embeddings/faiss.index
embeddings/docs.json
```

---

ğŸ¤– Step 2 â€” Run the RAG Chatbot

Mode 1 (recommended)

python -m src.pipeline


Mode 2 (direct)

python src/pipeline.py


---

ğŸ’¬ Usage

```
Ask: What does the assignment require?

Answer: The assignment requires you to...
```

To exit:

exit
quit
CTRL + C


---

ğŸ§© How it Works (RAG Architecture)

1. Ingestion

* PDF â†’ Pages â†’ Text
* Each page is converted into an embedding
* Stored in FAISS + JSON

2. Retrieval

* User question converted into embedding
* FAISS returns top-k similar pages

3. Generation

* Retrieved context passed to Mistral with:


[INST] question + context [/INST]

* Model generates final answer
 


ğŸ›  Technologies Used

* Python
* SentenceTransformers
* FAISS
* Mistral 7B Instruct
* llama.cpp
* PyPDF
* NumPy

---

ğŸ“ Future Improvements

* Chunk pages into smaller segments
* Stream output token-by-token
* Add a Gradio UI
* Add PDF upload option
* Evaluate retrieval accuracy

---

ğŸ‘¤ Author

Ritika Raj- (MCA) Data Science and AI



